# -*- coding: utf-8 -*-
"""sentence.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1la1OayXsXbvohl1MKec1xRI211TIWmZ3
"""

import pandas as pd
import tensorflow as tf
import pickle


with open('gpt.txt', 'r', encoding='ISO-8859-1') as f:
    file_content = f.read().lower()

#remove newlines and split into sentences
file_content = file_content.replace('\n', ' ')
print("Total Characters in the File:", len(file_content))


Tokenizer = tf.keras.preprocessing.text.Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts([file_content])
print("Vocabulary Size:", len(tokenizer.word_index))

input_sequences = []
for sentence in file_content.split('.'):  # Split by periods to get sentences
    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]
    for i in range(1, len(tokenized_sentence)):
        input_sequences.append(tokenized_sentence[:i + 1])

with open('tokenizer.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print("Tokenizer Saved!")

# Find the maximum sequence length
max_sequence_length = max([len(seq) for seq in input_sequences])
print("Maximum Sequence Length:", max_sequence_length)


pad_sequences = tf.keras.preprocessing.sequence.pad_sequences
padded_input_sequences = pad_sequences(input_sequences, padding='pre')
print("Shape of Padded Sequences:", padded_input_sequences.shape)

#split into X  and Y
X = padded_input_sequences[:, :-1]
Y = padded_input_sequences[:, -1]

#One-Hot Encode
to_categorical = tf.keras.utils.to_categorical
Y = to_categorical(Y, num_classes=len(tokenizer.word_index) + 1)

#the Model
Embedding = tf.keras.layers.Embedding
LSTM = tf.keras.layers.LSTM
Dense = tf.keras.layers.Dense
Sequential = tf.keras.models.Sequential

vocab_size = len(tokenizer.word_index) + 1
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=X.shape[1]))
model.add(LSTM(150))
model.add(Dense(vocab_size, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

#train
model.fit(X, Y, epochs=100, batch_size=32)


#Generate Text
def generate_sentence(seed_text, next_words=6):
    """
    Generate a sentence by predicting the next 'next_words' words based on the seed text.
    """
    for _ in range(next_words):
        # Tokenize the input text
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        # Pad the token list to match the input length
        token_list = pad_sequences([token_list], maxlen=X.shape[1], padding='pre')
        # Predict the next word
        predicted = model.predict(token_list, verbose=0)
        # Convert the predicted index back to a word
        predicted_word_index = tf.argmax(predicted, axis=-1).numpy()[0]
        predicted_word = tokenizer.index_word.get(predicted_word_index, '')


        seed_text += ' ' + predicted_word

    return seed_text



seed_text = "the meeting can be scheduled"
print("Generated Text:", generate_sentence(seed_text))

model.save("sentence_predictor.h5")
